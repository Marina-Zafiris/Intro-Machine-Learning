{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function and Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to go over the loss functions and gradient decent in relation to machine learning algorithms.\n",
    "In supervised machine learning algorithms, we want to minimize the error for each training example during the learning process. This error comes from the loss function. And the minimization is done using optimization strategies like gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines \"learn\" by means of a loss function. It's the standard method of how well a specific algorithm models it's given data. If the model's predictions deviates too much from the actual results, the loss/error function outputs a large number. \n",
    "\n",
    "However, there is no one-size-fits-all loss function in machine learning. There are various factors involved in choosing a loss function for specific problem such as type of machine learning algorithm chosen, ease of calculating the derivatives or even the number of potential outliers in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Regression Loss Functions\n",
    "Regression loss functions establish a linear relationship between a dependent variable (Y) and independent variable or variables (X). So we are trying to fit the best line in space on these variables.\n",
    "\n",
    "These are best utilized when you have *numerical/continous data*, for example predicting housing prices. However, they can be utilized for catagorical variables when utilizing the linear seperable algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Mean Squared Error Loss\n",
    "One of the most commonly used regression loss functions, MSE measures the average squared difference between the actual and predicted values by the model. The output is a single number associated with a set of values. \n",
    "Consider the slope intercept linear equation, $\\hat{y} = mx+b$. \n",
    "\n",
    "We derive MSE as:\n",
    "\n",
    "$ \\frac{1}{N}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PYTHON CODE FUNCTION FOR MSE\n",
    "# b is the intercept\n",
    "# m is the slope\n",
    "# points are from the data set\n",
    "\n",
    "def MSE(b, m, points):\n",
    "    N = float(len(points)) #N is the total number of points\n",
    "    error = 0\n",
    "    for i in range (0, len(points)):\n",
    "        x = points.iat[i, 0] #x values, note you could have multi-linear regression with multiple x's\n",
    "        y = points.iat[i, 1] #y values\n",
    "        error += (y-(m*x + b)) ** 2 #the squared summation of original y values minus predicted y_hat values\n",
    "    return error/N #the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Mean Squared Logarithmic Error Loss\n",
    "MSLE measures the ratio between actual and predicted value. It introduces a curve in the error curve. MSLE mainly focuses on percentual difference and predicted values. It can be a good choice as a loss function, when we want to predict house sales prices, bakery sales prices and the data is continuous.\n",
    "\n",
    "We derive MSLE as:\n",
    "\n",
    "$\\frac{1}{N}\\sum_{i=1}^{n}(log(y_{i}+1)-log(\\hat{y}_{i}+1))^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PYTHON CODE FUNCTION FOR MSE\n",
    "def MSLE(b, m, points):\n",
    "    N = float(len(points)) #N is the total number of points\n",
    "    error = 0\n",
    "    for i in range (0, len(points)):\n",
    "        x = points.iat[i, 0] #x values, note you could have multi-linear regression with multiple x's\n",
    "        y = points.iat[i, 1] #y values\n",
    "        log_y = math.log(y+1)\n",
    "        log_yhat = math.log((m*x + b) + 1)\n",
    "        error += (log_y - log_yhat) ** 2 #the squared summation of original y values minus predicted y_hat values\n",
    "    return error/N #the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Other Regression Errors\n",
    "Mean Absolute Error (MAE), Root mean squared error (RMSE), Huber Loss (combination of MSE and MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Binary Classification Loss Functions\n",
    "These loss functions are made to measure the performances of the classification model. In this, data points are assigned one of the labels i.e either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Binary Cross-Entropy Loss\n",
    "It’s a default loss function for binary classification problems. Cross-entropy loss calculates the performance of a classification model which gives an output of a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability value deviate from the actual label.\n",
    "Also called Sigmoid Cross-Entropy loss. It is a Sigmoid activation plus a Cross-Entropy loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i) Sigmoid\n",
    "It squashes a vector in the range (0, 1). It is applied independently to each element of $z$. It’s also called logistic function.\n",
    "\n",
    "Defined as:\n",
    "\n",
    "$\\sigma(z)= \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PYTHON CODE FOR SIGMOID FUNCTION\n",
    "## z is any value of which the sigmoid is individually applied\n",
    "### For example, z can be the y_hat regression equation\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii) Cross- Entropy\n",
    "We then apply that sigmoid to the Coss-Entropy loss function to obtain *Binary Cross-Entropy*.\n",
    "\n",
    "Defined as:\n",
    "\n",
    "$CE = -x_{i} log(f(\\sigma _{i})) - (1 - x_{i}) log(1 - f(\\sigma _{i}))$\n",
    "\n",
    "$x_{i}$ = 0 or 1\n",
    "\n",
    "We then take the derivative of the Cross-Entropy function paired with sigmoid in order to minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Other Binary Erros\n",
    "Hinge Loss, Square Hinge Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you venture into machine learning one of the fundamental aspects of your learning would be to understand “Gradient Descent”. Gradient descent is the backbone of an machine learning algorithm. Once you get hold of gradient descent things start to be more clear and it is easy to understand different algorithms. \n",
    "\n",
    "- 1) Choose the propper error function base on info listed above\n",
    "- 2) Take the derivative of the error function\n",
    "- 3) Give the function a starting point\n",
    "- 4) The point will either move in a positive or negative direction, depending on what loss function is being used as well at the learning rate\n",
    "- 5) The min/max is found after a number of iterations and the error is very close to zero\n",
    "\n",
    "\n",
    "I will be utilizing python to display examples of gradient decent with several different loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Basic Gradient Decent\n",
    "This is the simplest form of gradient descent technique. Its main feature is that we take small steps in the direction of the minima by taking gradient of the cost function.\n",
    "\n",
    "The gradient is in terms of each parameter needed for the final model.\n",
    "\n",
    "*PSUEDOCODE*\n",
    "<blockquote>\n",
    "    <p> update = learning_rate * gradient_of_parameters </p>\n",
    "    <p> parameters = parameters - update </p>\n",
    "</blockquote>\n",
    "\n",
    "Here, we see that we make an update to the parameters by taking gradient of the parameters. And multiplying it by a learning rate, which is essentially a constant number suggesting how fast we want to go the minimum. Learning rate is a hyper-parameter and should be treated with care when choosing its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Gradient Decent Using Basic Function\n",
    "In this example, we will take a function with multiple regressors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  4.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PYTHON CODE \n",
    "import numpy as np\n",
    "\n",
    "# Initializing variables\n",
    "x = [0.0, 10.0]\n",
    "\n",
    "#Original function, not used in the gradient function \n",
    "original_function = 5*((x[0])**2) + ((x[1])**2) + 4*(x[0])*(x[1]) - 6*x[0] - 4*x[1] + 15\n",
    "\n",
    "# Taking the fist derivative for each parameter\n",
    "gradient = np.array([10*(x[0]) + 4*(x[1]) - 6, 4*(x[0]) + 2*(x[1]) - 4])\n",
    "\n",
    "# Choosing our step parameter, often defined as alpha\n",
    "alpha = .16\n",
    "\n",
    "# Number of iterations\n",
    "num = 500\n",
    "\n",
    "def grad_decent(x, alpha, num):\n",
    "    gradient = np.array([10*(x[0]) + 4*(x[1]) - 6, 4*(x[0]) + 2*(x[1]) - 4]) #Initial Gradient\n",
    "    x_0 = x #Setting our starting point as the inital point\n",
    "    x_n = x_0 - alpha*gradient #creating a new x step point\n",
    "    i = 0\n",
    "    while i < num:\n",
    "        x_0 = x_n #Our inital point is now the step point just taken\n",
    "        gradient = np.array([10*(x_n[0]) + 4*(x_n[1]) - 6, 4*(x_n[0]) + 2*(x_n[1]) - 4]) #gradient of that step point\n",
    "        x_n = x_n - alpha*gradient #new step point created \n",
    "        i += 1\n",
    "    return x_n\n",
    "\n",
    "grad_decent(x, alpha, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Gradient Decent in application to cost functions\n",
    "As we explore several supervised algorithms we will come to find that gradient decent is the backbone to the vast majority of them. When hard coding a machine learning algorithms, the difficulty comes in being able to successfully take the derivative of our cost functions, some being quite complex, and vectorize that process through our code. The gradient decent method will be marked in the other notebooks through this Intro to Machine Learning Repository.\n",
    "Several packages within python have built in gradient decent functions for their associated algorithm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
